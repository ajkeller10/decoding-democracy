{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15603d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import create_test_data\n",
    "import boto3\n",
    "from unsupervised_topic_segmentation import dataset, core\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c7b13e1",
   "metadata": {},
   "source": [
    "# AMI data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54662b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#words to remove\n",
    "FILLERS=[\"um\", \"uh\", \"oh\", \"hmm\", \"mm-hmm\", \"uh-uh\", \"you know\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10da1fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ami loading\n",
    "topic_path='data/ami-and-icsi-corpora-master/ami-corpus/output/topics/'\n",
    "topic_jsons=create_test_data.jsons_to_dict_list(topic_path)\n",
    "\n",
    "#transcript ids\n",
    "ami_ids=[\"AMI_\"+str(x) for x in range(len(topic_jsons))]\n",
    "\n",
    "#jsons to clean dfs\n",
    "topic_dfs=[create_test_data.clean_topic_json(x,y,fillers=FILLERS) for x,y in zip(topic_jsons,ami_ids)]\n",
    "\n",
    "#df with one sentence per row\n",
    "df_ami=pd.concat(topic_dfs)\n",
    "\n",
    "#df with one transcript per row\n",
    "t_ami = pd.DataFrame({'transcript_id':ami_ids,\n",
    "                      'sentences':[tuple(x['sentences']) for x in topic_dfs],\n",
    "                      'length':[len(x['sentences']) for x in topic_dfs],\n",
    "                      'topic_count':[tuple(x['topic_count']) for x in topic_dfs],\n",
    "                      'mean_topic_length':[x.groupby(['topic_count']).size().mean() for x in topic_dfs],\n",
    "                      'topic_desc':[tuple(x['topic_desc']) for x in topic_dfs],\n",
    "                      'has_topic_desc':[tuple(x['has_topic_desc']) for x in topic_dfs]\n",
    "                      })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56f115e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to parquet\n",
    "df_ami.to_parquet('data/ami.parquet')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2afd3cab",
   "metadata": {},
   "source": [
    "# Transcript Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "553c3db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transcript loading\n",
    "t_cd = create_test_data.transcript_pickle_to_df()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89850073",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df with one sentence per row\n",
    "transcript_ids=list(t_cd['transcript_id'])\n",
    "sentences=list(t_cd['sentences'])\n",
    "df_transcripts=pd.concat([pd.DataFrame({'transcript_id':x,'sentences':y}) for x,y in zip(transcript_ids,sentences)])\n",
    "\n",
    "#clean transcripts\n",
    "df_transcripts=dataset.preprocessing(df_transcripts,'sentences',FILLERS,min_caption_len=20)\n",
    "df_transcripts=df_transcripts[[\"transcript_id\",\"sentences\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1fb8df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to parquet\n",
    "df_transcripts.to_parquet('data/transcripts.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1114af17",
   "metadata": {},
   "source": [
    "# Load to S3 - requires credential access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6613facf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the S3 bucket name and key for your CSV file\n",
    "bucket_name = 'decoding-democracy-embed'\n",
    "file_key = 'ami'\n",
    "\n",
    "# Create an S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Upload the CSV file to S3\n",
    "s3.upload_file('data/ami.parquet', bucket_name, 'ami.parquet')\n",
    "s3.upload_file('data/transcript.parquet', bucket_name, 'transcript.parquet')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c993c6a0",
   "metadata": {},
   "source": [
    "# Load Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0151f87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/ami_embed_results/part-00000-aa8e8005-9cf8-48a2-a561-e03cf4754b85-c000.snappy.parquet'\n",
    "df = create_test_data.snappy_parquet_to_df(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593e5e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['embeddings_len']=df['embeddings'].apply(lambda x: len(x))\n",
    "df['length']=df['sentences'].apply(lambda x: len(x))\n",
    "sum(df['length']!=df['embeddings_len'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
