{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15603d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import create_test_data\n",
    "import boto3\n",
    "from unsupervised_topic_segmentation import dataset, core\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c7b13e1",
   "metadata": {},
   "source": [
    "# AMI data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54662b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#words to remove\n",
    "FILLERS=[\"um\", \"uh\", \"oh\", \"hmm\", \"mm-hmm\", \"uh-uh\", \"you know\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10da1fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ami loading\n",
    "topic_path='data/ami-and-icsi-corpora-master/ami-corpus/output/topics/'\n",
    "topic_jsons=create_test_data.jsons_to_dict_list(topic_path)\n",
    "\n",
    "#transcript ids\n",
    "ami_ids=[\"AMI_\"+str(x) for x in range(len(topic_jsons))]\n",
    "\n",
    "#jsons to clean dfs\n",
    "topic_dfs=[create_test_data.clean_topic_json(x,y,fillers=FILLERS) for x,y in zip(topic_jsons,ami_ids)]\n",
    "\n",
    "#df with one sentence per row\n",
    "df_ami=pd.concat(topic_dfs)\n",
    "\n",
    "#df with one transcript per row\n",
    "t_ami = pd.DataFrame({'transcript_id':ami_ids,\n",
    "                      'sentences':[tuple(x['sentences']) for x in topic_dfs],\n",
    "                      'length':[len(x['sentences']) for x in topic_dfs],\n",
    "                      'topic_count':[tuple(x['topic_count']) for x in topic_dfs],\n",
    "                      'mean_topic_length':[x.groupby(['topic_count']).size().mean() for x in topic_dfs],\n",
    "                      'topic_desc':[tuple(x['topic_desc']) for x in topic_dfs],\n",
    "                      'has_topic_desc':[tuple(x['has_topic_desc']) for x in topic_dfs]\n",
    "                      })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "56f115e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to parquet\n",
    "df_ami.to_parquet('data/ami.parquet')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2afd3cab",
   "metadata": {},
   "source": [
    "# Transcript Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "553c3db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transcript loading\n",
    "t_cd = create_test_data.transcript_pickle_to_pd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "89850073",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df with one sentence per row\n",
    "transcript_ids=list(t_cd['transcript_id'])\n",
    "sentences=list(t_cd['sentences'])\n",
    "df_transcripts=pd.concat([pd.DataFrame({'transcript_id':x,'sentences':y}) for x,y in zip(transcript_ids,sentences)])\n",
    "\n",
    "#clean transcripts\n",
    "df_transcripts=dataset.preprocessing(df_transcripts,'sentences',FILLERS,min_caption_len=20)\n",
    "df_transcripts=df_transcripts[[\"transcript_id\",\"sentences\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a1fb8df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to parquet\n",
    "df_transcripts.to_parquet('data/transcripts.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1114af17",
   "metadata": {},
   "source": [
    "# Load to S3 - requires credential access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6613facf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the S3 bucket name and key for your CSV file\n",
    "bucket_name = 'decoding-democracy-embed'\n",
    "file_key = 'ami'\n",
    "\n",
    "# Create an S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Upload the CSV file to S3\n",
    "s3.upload_file('data/ami.parquet', bucket_name, 'ami.parquet')\n",
    "s3.upload_file('data/transcript.parquet', bucket_name, 'transcript.parquet')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c993c6a0",
   "metadata": {},
   "source": [
    "# Load Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0151f87c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript_id</th>\n",
       "      <th>sentences</th>\n",
       "      <th>topic_count</th>\n",
       "      <th>topic_desc</th>\n",
       "      <th>has_topic_desc</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AMI_0</td>\n",
       "      <td>[ Well , let's start .,  Okay . Okay . Not doi...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[opening, opening, opening, opening, opening, ...</td>\n",
       "      <td>[True, True, True, True, True, True, True, Tru...</td>\n",
       "      <td>[[tensor(0.0949), tensor(-0.3398), tensor(0.09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AMI_1</td>\n",
       "      <td>[This is our third meeting already ., I hope y...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[opening, opening, opening, opening, opening, ...</td>\n",
       "      <td>[True, True, True, True, True, True, True, Tru...</td>\n",
       "      <td>[[tensor(0.1004), tensor(-0.0374), tensor(0.03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AMI_10</td>\n",
       "      <td>[welcome everyone to our next meeting ., and b...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, ...</td>\n",
       "      <td>[opening, opening, opening, opening, opening, ...</td>\n",
       "      <td>[True, True, True, True, True, True, True, Tru...</td>\n",
       "      <td>[[tensor(-0.0851), tensor(-0.1397), tensor(0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AMI_100</td>\n",
       "      <td>[ Wouldn't wanna be Project Manager .,   , wha...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[opening, opening, opening, opening, opening, ...</td>\n",
       "      <td>[True, True, True, True, True, True, True, Tru...</td>\n",
       "      <td>[[tensor(0.1179), tensor(-0.0844), tensor(0.03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AMI_101</td>\n",
       "      <td>[let's start our second me meeting on  concept...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[opening, opening, opening, opening, opening, ...</td>\n",
       "      <td>[True, True, True, True, True, True, True, Tru...</td>\n",
       "      <td>[[tensor(0.0908), tensor(0.0328), tensor(0.088...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>AMI_95</td>\n",
       "      <td>[And then you have to place your laptop exactl...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[agenda/equipment issues, agenda/equipment iss...</td>\n",
       "      <td>[True, True, True, True, True, True, True, Tru...</td>\n",
       "      <td>[[tensor(-0.0273), tensor(0.0039), tensor(0.03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>AMI_96</td>\n",
       "      <td>[Sorry I'm a little late ., Got stuck in the t...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[opening, opening, opening, opening, opening, ...</td>\n",
       "      <td>[True, True, True, True, True, True, True, Tru...</td>\n",
       "      <td>[[tensor(0.0498), tensor(-0.1051), tensor(0.16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>AMI_97</td>\n",
       "      <td>[One , two , three , four ,, Welcome to this s...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[agenda/equipment issues, opening, opening, op...</td>\n",
       "      <td>[True, True, True, True, True, True, True, Tru...</td>\n",
       "      <td>[[tensor(0.1273), tensor(-0.1799), tensor(0.13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>AMI_98</td>\n",
       "      <td>[being as a Marketing Exper Expert I will like...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[discussion, discussion, discussion, discussio...</td>\n",
       "      <td>[True, True, True, True, True, True, True, Tru...</td>\n",
       "      <td>[[tensor(-0.0048), tensor(0.0419), tensor(0.03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>AMI_99</td>\n",
       "      <td>[   minutes from the last meeting which were e...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[opening, opening, opening, opening, opening, ...</td>\n",
       "      <td>[True, True, True, True, True, True, True, Tru...</td>\n",
       "      <td>[[tensor(0.0156), tensor(0.0014), tensor(0.098...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    transcript_id                                          sentences  \\\n",
       "0           AMI_0  [ Well , let's start .,  Okay . Okay . Not doi...   \n",
       "1           AMI_1  [This is our third meeting already ., I hope y...   \n",
       "2          AMI_10  [welcome everyone to our next meeting ., and b...   \n",
       "3         AMI_100  [ Wouldn't wanna be Project Manager .,   , wha...   \n",
       "4         AMI_101  [let's start our second me meeting on  concept...   \n",
       "..            ...                                                ...   \n",
       "131        AMI_95  [And then you have to place your laptop exactl...   \n",
       "132        AMI_96  [Sorry I'm a little late ., Got stuck in the t...   \n",
       "133        AMI_97  [One , two , three , four ,, Welcome to this s...   \n",
       "134        AMI_98  [being as a Marketing Exper Expert I will like...   \n",
       "135        AMI_99  [   minutes from the last meeting which were e...   \n",
       "\n",
       "                                           topic_count  \\\n",
       "0    [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, ...   \n",
       "3    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4    [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "..                                                 ...   \n",
       "131  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "132  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "133  [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "134  [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "135  [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                            topic_desc  \\\n",
       "0    [opening, opening, opening, opening, opening, ...   \n",
       "1    [opening, opening, opening, opening, opening, ...   \n",
       "2    [opening, opening, opening, opening, opening, ...   \n",
       "3    [opening, opening, opening, opening, opening, ...   \n",
       "4    [opening, opening, opening, opening, opening, ...   \n",
       "..                                                 ...   \n",
       "131  [agenda/equipment issues, agenda/equipment iss...   \n",
       "132  [opening, opening, opening, opening, opening, ...   \n",
       "133  [agenda/equipment issues, opening, opening, op...   \n",
       "134  [discussion, discussion, discussion, discussio...   \n",
       "135  [opening, opening, opening, opening, opening, ...   \n",
       "\n",
       "                                        has_topic_desc  \\\n",
       "0    [True, True, True, True, True, True, True, Tru...   \n",
       "1    [True, True, True, True, True, True, True, Tru...   \n",
       "2    [True, True, True, True, True, True, True, Tru...   \n",
       "3    [True, True, True, True, True, True, True, Tru...   \n",
       "4    [True, True, True, True, True, True, True, Tru...   \n",
       "..                                                 ...   \n",
       "131  [True, True, True, True, True, True, True, Tru...   \n",
       "132  [True, True, True, True, True, True, True, Tru...   \n",
       "133  [True, True, True, True, True, True, True, Tru...   \n",
       "134  [True, True, True, True, True, True, True, Tru...   \n",
       "135  [True, True, True, True, True, True, True, Tru...   \n",
       "\n",
       "                                            embeddings  \n",
       "0    [[tensor(0.0949), tensor(-0.3398), tensor(0.09...  \n",
       "1    [[tensor(0.1004), tensor(-0.0374), tensor(0.03...  \n",
       "2    [[tensor(-0.0851), tensor(-0.1397), tensor(0.0...  \n",
       "3    [[tensor(0.1179), tensor(-0.0844), tensor(0.03...  \n",
       "4    [[tensor(0.0908), tensor(0.0328), tensor(0.088...  \n",
       "..                                                 ...  \n",
       "131  [[tensor(-0.0273), tensor(0.0039), tensor(0.03...  \n",
       "132  [[tensor(0.0498), tensor(-0.1051), tensor(0.16...  \n",
       "133  [[tensor(0.1273), tensor(-0.1799), tensor(0.13...  \n",
       "134  [[tensor(-0.0048), tensor(0.0419), tensor(0.03...  \n",
       "135  [[tensor(0.0156), tensor(0.0014), tensor(0.098...  \n",
       "\n",
       "[136 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'data/ami_embed_results/part-00000-aa8e8005-9cf8-48a2-a561-e03cf4754b85-c000.snappy.parquet'\n",
    "df = create_test_data.snappy_parquet_to_df(filename)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "593e5e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['embeddings_len']=df['embeddings'].apply(lambda x: len(x))\n",
    "df['length']=df['sentences'].apply(lambda x: len(x))\n",
    "sum(df['length']!=df['embeddings_len'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fd903d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
