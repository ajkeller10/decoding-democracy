{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "4c728414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55856e81",
   "metadata": {},
   "source": [
    "# import data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "227640d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"transcripts.pickle\", \"rb\") as f:\n",
    "    transcripts = pickle.load(f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0294039f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of transcripts: 412\n",
      "Number of sentencess: 328733\n",
      "Number of words: 4843941\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of transcripts: {len(transcripts)}\")\n",
    "print(f\"Number of sentencess: {sum([len(sentences) for sentences in transcripts.values()])}\")\n",
    "print(f\"Number of words: {sum([sum([len(sentence.split()) for sentence in sentences]) for sentences in transcripts.values()])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f5b1bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thank you.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex=list(transcripts.keys())[0]\n",
    "transcripts[ex][-1]\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50342cef",
   "metadata": {},
   "source": [
    "# clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "d88dea3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.DataFrame(transcripts.keys(),columns=[\"transcript_id\"])\n",
    "\n",
    "t['sentences']=t[\"transcript_id\"].apply(lambda x: transcripts[x])\n",
    "t['length']=t['sentences'].apply(lambda x: len(x)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2097aeb1",
   "metadata": {},
   "source": [
    "# generate segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "f7b670c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_segment(t : pd, doc_count_limit: int = 10, sentence_min: int = 20) -> tuple: \n",
    "    '''\n",
    "    generate text segment from transcript data that resembles a transcript using 1:doc_count_limit documents \n",
    "    \n",
    "    Args:\n",
    "        t: transcripts\n",
    "        doc_count_limit: maximum number of docs to pull from\n",
    "        sentence_min: minimum number of sentences per text\n",
    "    \n",
    "    Returns:\n",
    "        segment (list): List of sentences\n",
    "        labels (list): Number document sentence pulled from \n",
    "        \n",
    "    \n",
    "    '''\n",
    "\n",
    "    #filter text for sufficiently long texts \n",
    "    t_long=t[t['length']>=(doc_count_limit*sentence_min)]\n",
    "\n",
    "    #count of documents\n",
    "    doc_count = random.randint(1,doc_count_limit)\n",
    "    \n",
    "    #pull documents based on document count\n",
    "    text = t_long.sample(doc_count)\n",
    "\n",
    "    #add row number as index\n",
    "    text.reset_index(inplace=True,drop=True)\n",
    "    text.reset_index(inplace=True)\n",
    "    \n",
    "    if doc_count>1:\n",
    "\n",
    "        #get length of last document to be total length of new document\n",
    "        #NOTE: this makes output length independent of doc_count\n",
    "        length_sequence = text.iat[-1,int(text.columns.get_indexer(['length']))]\n",
    "\n",
    "        #distribute sentences between documents - based on percentage of chosen length\n",
    "        percents = np.random.randint(1,100, size=doc_count)\n",
    "        text['sentences_to_use'] = length_sequence * percents / sum(percents)\n",
    "        text['sentences_to_use']= text['sentences_to_use'].astype('int') + sentence_min\n",
    "\n",
    "        #reallocate sentences from shorter documents to last documents if necessary\n",
    "        #NOTE: this lets us incorporate smaller documents, rather than oversampling from larger documents\n",
    "        text['reallocate']=text['length']-text['sentences_to_use']\n",
    "        text.loc[text['reallocate']>0,'reallocate'] = 0\n",
    "        text['sentences_to_use']=text['sentences_to_use']+text['reallocate']\n",
    "        text['reallocate_cum']=text['reallocate'].cumsum()\n",
    "\n",
    "        #decide where to pull the sentences from \n",
    "        text['sentences_start']=text.apply(lambda x: random.randint(0,x['length']-x['sentences_to_use']),axis=1)\n",
    "\n",
    "        #the first segment start at 0, and the last end at -1\n",
    "        text.iat[-1,int(text.columns.get_indexer(['sentences_start']))]=text.iloc[-1]['length']-text.iloc[-1]['sentences_to_use']+text.iloc[-1]['reallocate_cum']    \n",
    "        #text.loc[text['sentences_start']<0,'sentences_start'] = 0 #prevent overflow: should only needed when doc_count=1\n",
    "        text.iat[0,int(text.columns.get_indexer(['sentences_start']))]=0\n",
    "    \n",
    "    else: \n",
    "        text['sentences_to_use'] = text['length'] \n",
    "        text['sentences_start']=0\n",
    "       \n",
    "    #get sentences\n",
    "    text['results']=text.apply(lambda x: x['sentences'][x.sentences_start:(x.sentences_start+x.sentences_to_use)],axis=1) #)\n",
    "    text['labels']=text.apply(lambda x: [x['index']] * x['sentences_to_use'],axis=1)\n",
    "    \n",
    "    results = list(itertools.chain.from_iterable(text['results']))\n",
    "    labels = list(itertools.chain.from_iterable(text['results']))\n",
    "\n",
    "    return (results,labels,doc_count)\n",
    "    \n",
    "hold = generate_segment(t, doc_count_limit = 10, sentence_min = 20)  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
